version: '3.5'

networks:
  hadoop_cluster:
    name: hadoop_cluster
    driver: bridge

services:
  namenode:
    image: ashokkumarchoppadandi/apache-hadoop-hive:2.7.6-2.3.8
    container_name: namenode
    networks:
      - hadoop_cluster
    ports:
      - "50070:50070"
    hostname: namenode
    environment:
      FS_DEFAULT_NAME: hdfs:\/\/namenode:9000
      DFS_REPLICATION: 1
      YARN_RESOURCEMANAGER_HOSTNAME: resourcemanager
      HISTORY_SERVER_HOST: historyserver
    command: namenode

  secondarynamenode:
    image: ashokkumarchoppadandi/apache-hadoop-hive:2.7.6-2.3.8
    container_name: secondarynamenode
    networks:
      - hadoop_cluster
    depends_on:
      - namenode
    ports:
      - "50030:50030"
    hostname: secondarynamenode
    environment:
      FS_DEFAULT_NAME: hdfs:\/\/namenode:9000
      DFS_REPLICATION: 1
      YARN_RESOURCEMANAGER_HOSTNAME: resourcemanager
      HISTORY_SERVER_HOST: historyserver
    command: secondarynamenode

  resourcemanager:
    image: ashokkumarchoppadandi/apache-hadoop-hive:2.7.6-2.3.8
    container_name: resourcemanager
    networks:
      - hadoop_cluster
    depends_on:
      - namenode
      - secondarynamenode
    ports:
      - "8088:8088"
    hostname: resourcemanager
    environment:
      FS_DEFAULT_NAME: hdfs:\/\/namenode:9000
      DFS_REPLICATION: 1
      YARN_RESOURCEMANAGER_HOSTNAME: resourcemanager
      HISTORY_SERVER_HOST: historyserver
    command: resourcemanager

  historyserver:
    image: ashokkumarchoppadandi/apache-hadoop-hive:2.7.6-2.3.8
    container_name: historyserver
    networks:
      - hadoop_cluster
    depends_on:
      - namenode
      - secondarynamenode
      - resourcemanager
    ports:
      - "19888:19888"
    hostname: historyserver
    environment:
      FS_DEFAULT_NAME: hdfs:\/\/namenode:9000
      DFS_REPLICATION: 1
      YARN_RESOURCEMANAGER_HOSTNAME: resourcemanager
      HISTORY_SERVER_HOST: historyserver
    command: historyserver

  mysql:
    image: ashokkumarchoppadandi/mysql:latest
    container_name: mysql
    networks:
      - hadoop_cluster
    hostname: mysql
    environment:
      MYSQL_ROOT_PASSWORD: Password@123
      MYSQL_DATABASE: metastore

  postgres:
    image: postgres:13
    container_name: postgres
    hostname: postgres
    networks:
      - hadoop_cluster
    environment:
      POSTGRES_DB: kafkadb
      POSTGRES_USER: hadoop
      POSTGRES_PASSWORD: Password@123
    ports:
      - "5432:5432"

  slavenode:
    image: ashokkumarchoppadandi/apache-hadoop-hive:2.7.6-2.3.8
    container_name: slavenode
    networks:
      - hadoop_cluster
    depends_on:
      - namenode
      - secondarynamenode
      - resourcemanager
      - historyserver
      - mysql
    ports:
      - "18042:18042"
      - "35945:35945"
      - "4040:4040"
      - "10002:10002"
    hostname: slavenode
    environment:
      FS_DEFAULT_NAME: hdfs:\/\/namenode:9000
      DFS_REPLICATION: 1
      YARN_RESOURCEMANAGER_HOSTNAME: resourcemanager
      HISTORY_SERVER_HOST: historyserver
      JAVAX_JDO_OPTION_CONNECTIONURL: jdbc:mysql:\/\/mysql\/metastore?createDatabaseIfNotExist=true
    command:
      - slavenode
      - hive

  zookeeper:
    image: ashokkumarchoppadandi/confluent-kafka:5.5.3
    container_name: zookeeper
    networks:
      - hadoop_cluster
    ports:
      - "2181:2181"
    hostname: zookeeper
    environment:
      ZOOKEEPER_DATA_DIR: \/tmp\/zookeeper
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_MAX_CLIENT_CONNECTIONS: 0
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_ENABLE_SERVER: "false"
      ZOOKEEPER_ADMIN_SERVER_PORT: 8080
    command:
      - zookeeper
      - standalone

  broker:
    image: ashokkumarchoppadandi/confluent-kafka:5.5.3
    container_name: broker
    networks:
      - hadoop_cluster
    depends_on:
      - zookeeper
    hostname: broker
    environment:
      BROKER_ID: 0
      KAFKA_LISTENERS: PLAINTEXT:\/\/0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT:\/\/broker:9092
      NUM_NETWORK_THREADS: 3
      NUM_IO_THREADS: 8
      SOCKET_SEND_BUFFER_BYTES: 102400
      SOCKET_RECEIVE_BUFFER_BYTES: 102400
      SOCKET_REQUEST_MAX_BYTES: 104857600
      LOG_DIRS: \/tmp\/kafka-logs
      NUM_PARTITIONS: 1
      NUM_RECOVERY_THREADS_PER_DATA_DIR: 1
      OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      TRANSACTION_STATE_LOG_MIN_ISR: 1
      LOG_RETENTION_HOURS: 168
      LOG_SEGMENT_BYTES: 1073741824
      LOG_RETENTION_CHECK_INTERVAL_MS: 300000
      ZOOKEEPER_CONNECT_LIST: "zookeeper:2181"
      ZOOKEEPER_CONNECTION_TIMEOUT_MS: 18000
      GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    ports:
      - "9092:9092"
    command: kafka

  schemaregistry:
    image: ashokkumarchoppadandi/confluent-kafka:5.5.3
    container_name: schemaregistry
    networks:
      - hadoop_cluster
    depends_on:
      - zookeeper
      - broker
    hostname: schemaregistry
    environment:
      SCHEMA_REGISTRY_LISTENERS: http:\/\/0.0.0.0:8081
      KAFKASTORE_BOOTSTRAP_SERVERS: "broker:9092"
      KAFKASTORE_TOPIC: "_schemas"
      DEBUG: "true"
    ports:
      - "8081:8081"
    command: schemaregistry

  connect:
    image: ashokkumarchoppadandi/confluent-kafka:5.5.3
    container_name: connect
    networks:
      - hadoop_cluster
    depends_on:
      - zookeeper
      - broker
      - schemaregistry
    hostname: connect
    environment:
      CONNECT_WITH_AVRO_DISTRIBUTED: "true"
      BOOTSTRAP_SERVERS: broker:9092
      GROUP_ID: connect-cluster
      KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      KEY_CONVERTER_SCHEMA_REGISTRY_URL: http:\/\/schemaregistry:8081
      VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http:\/\/schemaregistry:8081
      KEY_CONVERTER_SCHEMAS_ENABLE: "true"
      VALUE_CONVERTER_SCHEMAS_ENABLE: "true"
      OFFSET_STORAGE_TOPIC: connect-offsets
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      OFFSET_STORAGE_PARTITIONS: 25
      CONFIG_STORAGE_TOPIC: connect-configs
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_TOPIC: connect-status
      STATUS_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_PARTITIONS: 5
      OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_REST_HOST: 0.0.0.0
      CONNECT_REST_PORT: 8083
      CONNECT_REST_ADVERTISED_HOST: connect
      CONNECT_REST_ADVERTISED_PORT: 8083
      PLUGIN_PATH: \/usr\/local\/confluent\/share\/java
    ports:
      - "8083:8083"
    command: connect

  ksql:
    image: ashokkumarchoppadandi/confluent-kafka:5.5.3
    container_name: ksql
    networks:
      - hadoop_cluster
    depends_on:
      - zookeeper
      - broker
      - schemaregistry
      - connect
    hostname: ksql
    environment:
      BOOTSTRAP_SERVERS: "broker:9092"
      KSQL_DB_LISTENERS: http:\/\/0.0.0.0:8088
      KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: "true"
      KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: "true"
      KSQL_LOGGING_PROCESSING_ROWS_INCLUDE: "true"
      KSQL_SCHEMA_REGISTRY_URL: http:\/\/schemaregistry:8081
      UI_ENABLED: "true"
      KSQL_STREAMS_AUTO_OFFSET_RESET: latest
      KSQL_STREAMS_COMMIT_INTERVAL_MS: 2000
      KSQL_STREAMS_CACHE_MAX_BYTES_BUFFERING: 10000000
      KSQL_FAIL_ON_DESERIALIZATION_ERROR: "true"
      KSQL_STREAMS_NUM_STREAM_THREADS: 1
      KSQL_SERVICE_ID: default_
      KSQL_SINK_PARTITIONS: 4
      KSQL_SINK_REPLICAS: 1
      KSQL_KSQL_FUNCTIONS_FORMULA_BASE_VALUE: 5
      KSQL_KSQL_EXTENSION_DIR: \/usr\/local\/confluent\/ext
      KSQL_KSQL_CONNECT_URL: http:\/\/connect:8083
    ports:
      - "18088:8088"
    command: ksql
    volumes:
      - ./KSQL-UDFs:/usr/local/confluent/ext:rw
  edgar_logs_producer:
    image: ashokkumarchoppadandi/edgar-logs-kafka-producer:1.0-SNAPSHOT
    hostname: edgar_logs_producer
    container_name: edgar_logs_producer
    networks:
      - hadoop_cluster
    depends_on:
      - broker
      - schemaregistry
    environment:
      BOOTSTRAP_SERVERS: broker:9092
      ACKS: all
      RETRIES: 0
      BATCH_SIZE: 16384
      LINGER_MS: 1
      BUFFER_MEMORY: 33554432
      KEY_SERIALIZER: org.apache.kafka.common.serialization.StringSerializer
      VALUE_SERIALIZER: io.confluent.kafka.serializers.KafkaAvroSerializer
      SCHEMA_REGISTRY_URL: http:\/\/schemaregistry:8081
      SCHEMA_FILE_LOCATION: \/edgar\/edgar.avsc
      OUTPUT_FORMAT: avro
      TOPIC_NAME: logs-test
      INPUT_LOGS_DIR: \/edgar\/input_logs\/
      IS_CONFIGURING_PROPERTIES_REQUIRED: "yes"
      IS_KERBEROS_AUTHENTICATION_REQUIRED: "no"
      JAR_FILE_LOCATION: /java_examples/
      JAR_NAME: KafkaExamples-1.0-SNAPSHOT.jar
      RUN_CLASS: com.bigdata.kafka.producer.file.dynamic.EdgarLogsKafkaDynamicFileProducer
    volumes:
      - /c/Users/lenovo/IdeaProjects/StreamingExamples/KafkaExamples/src/main/resources/input_logs/:/edgar/input_logs/:rw

  elasticsearch01:
    image: ashokkumarchoppadandi/elasticsearch:7.17.0
    hostname: elasticsearch01
    container_name: elasticsearch01
    networks:
      - hadoop_cluster
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      HOSTNAME: elasticsearch01
      ES_CLUSTER_NAME: my-elasticsearch
      ES_NODE_NAME: elasticsearch01
      ES_NODE_MASTER: "true"
      ES_DATA_PATH: "/usr/local/elasticsearch/data/"
      ES_LOGS_PATH: "/usr/local/elasticsearch/logs/"
      ES_NETWORK_HOST: 0.0.0.0
      ES_HTTP_PORT: 9200
      ES_BOOTSTRAP_MEMORY_LOCK: "true"
      ES_DISCOVERY_SEED_HOSTS: "127.0.0.1, [::1]"
      ES_MASTER_NODES: elasticsearch01
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
  kibana01:
    image: ashokkumarchoppadandi/kibana:7.17.0
    hostname: kibanahost1
    container_name: kibanahost1
    depends_on:
      - elasticsearch01
    networks:
      - hadoop_cluster
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_HOSTS: "http://elasticsearch01:9200"
      KIBANA_SERVER_PORT: 5601
      KIBANA_SERVER_HOST: 0.0.0.0
      KIBANA_SERVER_NAME: kibanahost1