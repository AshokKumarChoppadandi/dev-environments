INSTALLING HADOOP


1. Update the OS

	sudo yum update -y

2. Install Java 8

	sudo yum install java-1.8.0-openjdk-devel -y

3. Set JAVA_HOME
	
	vi ~/.bashrc

	# JAVA_HOME
	export JAVA_HOME=/usr/lib/jvm/java
	export PATH=$PATH:$JAVA_HOME/bin

4. Setup static IP

	sudo vi /etc/sysconfig/network-scripts/ifcfg-enp0s3

	TYPE="Ethernet"
	PROXY_METHOD="none"
	BROWSER_ONLY="no"
	BOOTPROTO="static"
	DEFROUTE="yes"
	IPV4_FAILURE_FATAL="no"
	IPV6INIT="yes"
	IPV6_AUTOCONF="yes"
	IPV6_DEFROUTE="yes"
	IPV6_FAILURE_FATAL="no"
	IPV6_ADDR_GEN_MODE="stable-privacy"
	NAME="enp0s3"
	UUID="5cc67637-1270-40ce-b066-6cb191fc3e48"
	DEVICE="enp0s3"
	ONBOOT="yes"
	IPADDR="192.168.0.211"
	NETMASK="255.255.255.0"
	GATEWAY="192.168.0.1"
	DNS="8.8.8.8"


5. Password Less Login (Same host if 1 node)

	ssh-keygen -t rsa

	ssh-copy-id -i ~/.ssh/id_rsa.pub centos@192.168.0.211

6. Disable strict Host checking

	vi ~/.ssh/ssh-config

	Host *
	  UserKnownHostsFile /dev/null
	  StrictHostKeyChecking no

7. Diable Firewall

	systemctl status firewalld

	systemctl stop firewalld

	systemctl disable firewalld

8. Download Hadoop, Untar & Create a Soft Link

	wget https://archive.apache.org/dist/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz

	sudo tar -xzvf hadoop-2.7.7.tar.gz -C /usr/local/

	ln -s /usr/local/hadoop-2.7.7 /usr/local/hadoop

9. Set HADOOP_HOME & Add binaries to PATH

	# HADOOP_HOME
	export HADOOP_HOME=/usr/local/hadoop
	export PATH=$PATH:$HADOOP_HOME/bin

10. Set JAVA_HOME in hadoop-env.sh

	export JAVA_HOME=/usr/lib/jvm/java

11. Edit configuration files

	vi $HADOOP_HOME/etc/hadoop/core-site.xml

	<?xml version="1.0" encoding="UTF-8"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<configuration>
	    <property>
	        <name>hadoop.tmp.dir</name>
	        <value>/usr/local/hadoop/data/tmp</value>
	    </property>
	    <property>
	        <name>fs.defaultFS</name>
	        <value>hdfs://192.168.0.211:9000</value>
	    </property>
	</configuration>

	vi $HADOOP_HOME/etc/hadoop/hdfs-site.xml

	<?xml version="1.0" encoding="UTF-8"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<configuration>
	    <property>
	            <name>dfs.namenode.name.dir</name>
	            <value>/usr/local/hadoop/data/namenode</value>
	    </property>
	    <property>
	            <name>dfs.datanode.data.dir</name>
	            <value>/usr/local/hadoop/data/datanode</value>
	    </property>
	    <property>
	      <name>dfs.namenode.checkpoint.dir</name>
	      <value>/usr/local/hadoop/data/namesecondary</value>
	    </property>    
	    <property>
	            <name>dfs.replication</name>
	            <value>1</value>
	    </property>
	    <property>
	            <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
	            <value>false</value>
	    </property>
	</configuration>

	vi $HADOOP_HOME/etc/hadoop/yarn-site.xml

	<?xml version="1.0"?>
	<configuration>
		<property>
			<name>yarn.acl.enable</name>
			<value>0</value>
		</property>
		<property>
			<name>yarn.resourcemanager.hostname</name>
			<value>HOSTNAME</value>
		</property>
		<property>
			<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
			<value>org.apache.hadoop.mapred.ShuffleHandler</value>
		</property>
		<property>
			<name>yarn.resourcemanager.scheduler.address</name>
			<value>HOSTNAME:8030</value>
		</property>
		<property>
			<name>yarn.resourcemanager.address</name>
			<value>HOSTNAME:8032</value>
		</property>
		<property>
			<name>yarn.resourcemanager.webapp.address</name>
			<value>HOSTNAME:8088</value>
		</property>
		<property>
			<name>yarn.resourcemanager.resource-tracker.address</name>
			<value>HOSTNAME:8031</value>
		</property>
		<property>
			<name>yarn.resourcemanager.admin.address</name>
			<value>HOSTNAME:8033</value>
		</property> 
		<property>
			<name>yarn.nodemanager.aux-services</name>
			<value>mapreduce_shuffle</value>
		</property>
		<property>
		 	<name>yarn.nodemanager.resource.memory-mb</name>
		 	<value>4096</value>
		</property>
		<property>
		 	<name>yarn.scheduler.maximum-allocation-mb</name>
		 	<value>4096</value>
		</property>
		<property>
			<name>yarn.scheduler.minimum-allocation-mb</name>
			<value>512</value>
		</property>
		<property>
			<name>yarn.nodemanager.resource.cpu-vcores</name>
			<value>2</value>
		</property>  
		<property>
			<name>yarn.nodemanager.vmem-check-enabled</name>
			<value>false</value>
		</property>
		<property>
	    	<name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
    		<value>95</value>
		</property>
		<property>
			<name>yarn.nodemanager.pmem-check-enabled</name>
			<value>false</value>
		</property>
	</configuration>

	vi $HADOOP_HOME/etc/hadoop/mapred-site.xml

	<?xml version="1.0"?>
	<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
	<configuration>
	  <property>
	    <name>mapreduce.framework.name</name>
	    <value>yarn</value>
	  </property>
	  <property>
	    <name>mapreduce.job.tracker</name>
	    <value>HOSTNAME:9001</value>
	  </property>
	  <property>
	    <name>mapreduce.jobhistory.webapp.address</name>
	    <value>HOSTNAME:19888</value>
	  </property>
	  <property>
	    <name>yarn.nodemanager.resource.memory-mb</name>
	    <value>4096</value>
	  </property>
	  <property>
	    <name>yarn.scheduler.maximum-allocation-mb</name>
	    <value>4096</value>
	  </property>
	  <property>
	    <name>yarn.scheduler.minimum-allocation-mb</name>
	    <value>512</value>
	  </property>
	  <property>
	    <name>yarn.nodemanager.vmem-check-enabled</name>
	    <value>false</value>
	  </property>
	  <property>
	    <name>yarn.app.mapreduce.am.resource.mb</name>
	    <value>512</value>
	  </property>
	  <property>
	    <name>yarn.app.mapreduce.am.command-opts</name>
	    <value>-Xmx2048m</value>
	  </property>
	  <property>
	    <name>mapreduce.map.cpu.vcores</name>
	    <value>1</value>
	  </property>
	  <property>
	    <name>mapreduce.reduce.cpu.vcores</name>
	    <value>1</value>
	  </property>
	</configuration>

12. Create tmp, namenode, datanode directories

13. Export HDFS_NAMENODE_USER, HDFS_DATANODE_USER, HDFS_SECONDARYNAMENODE_USER, HADOOP_CONF_DIR

	# HADOOP_CONF_DIR
	export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop

	export HADOOP_HOME=/usr/local/hadoop
	export HADOOP_COMMON_HOME=$HADOOP_HOME
	export HADOOP_HDFS_HOME=$HADOOP_HOME
	export HADOOP_MAPRED_HOME=$HADOOP_HOME
	export HADOOP_YARN_HOME=$HADOOP_HOME
	export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
	export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
	export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin

14. Format Namenode

15. Start Namenode

16. Start Secondary Namenode

17. Start Data Node

18. Start Resource Manager

19. Start Node Manager

20. Start MR History Server

21. Check the services with jps command

22. Check the services by opening respective WEB URLs

	localhost:50070		Namenode
	localhost:50090		Secondary Namenode
	localhost:8088		Resource Manager
	localhost:19888		History Server

23. Check the Services are working fine or not by executing the PI program

	yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar pi 10 10

